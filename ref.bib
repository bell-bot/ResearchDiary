@inproceedings{Mller2022,
   abstract = {Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github.com/automl/TransformersCanDoBayesianInference.},
   author = {Samuel Müller and Noah Hollmann and Sebastian Pineda and Josif Grabocka and Frank Hutter},
   booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
   title = {TRANSFORMERS CAN DO BAYESIAN INFERENCE},
   year = {2022}
}
@article{Agarwal2026,
   abstract = {Modern sequence models often appear to behave as Bayesian learners, but it remains unclear whether this reflects genuine probabilistic inference or task-specific heuristics. We introduce Bayesian wind tunnels-controlled environments where the true posterior is known in closed form and memorization is provably impossible-to resolve this question empirically. In these settings, small transformers reproduce exact Bayesian posteriors for filtering and hypothesis elimination with 10 −3-10 −4 bit accuracy, while capacity-matched MLPs fail by orders of magnitude. To understand which architectural ingredients enable exact inference, we decompose Bayesian computation into three inference primitives: (i) belief accumulation-integrating evidence into a running posterior; (ii) belief transport-propagating beliefs forward through stochastic dynamics; and (iii) random-access binding-retrieving stored hypotheses by content rather than position. Different tasks demand different subsets of these primitives, and different architectures can realize different subsets. Comparing Transformers, Mamba, LSTMs, and MLPs across bijection learning, HMM filtering, and associative recall, we find that Transformers realize all three primitives; Mamba realizes accumulation and transport but struggles with random-access binding; LSTMs realize only accumulation (of static sufficient statistics); and MLPs realize none. Geometric diagnostics reveal orthogonal key bases, low-dimensional value manifolds parameterized by posterior entropy, and-in Mamba-five discrete clusters corresponding to HMM hidden states. These results demonstrate that Bayesian computation is not monolithic: its realizability depends on the inference primitives a task demands and the architectural mechanisms available to implement them. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.},
   author = {Naman Agarwal and Siddhartha R Dalal},
   keywords = {cs.AI,cs.LG,stat.ML},
   title = {The Bayesian Geometry of Transformer Attention Paper I of the Bayesian Attention Trilogy},
   volume = {1},
   url = {https://arxiv.org/abs/2512.22471},
   year = {2026}
}
@article{Hoffman2014,
   abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size e and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter e on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers. © 2014 Matthew D. Hoffman and Andrew Gelman.},
   author = {Matthew D. Hoffman and Andrew Gelman},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   title = {The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo},
   volume = {15},
   year = {2014}
}
@misc{Achim2025,
      title={Aristotle: IMO-level Automated Theorem Proving}, 
      author={Tudor Achim and Alex Best and Alberto Bietti and Kevin Der and Mathïs Fédérico and Sergei Gukov and Daniel Halpern-Leistner and Kirsten Henningsgard and Yury Kudryashov and Alexander Meiburg and Martin Michelsen and Riley Patterson and Eric Rodriguez and Laura Scharff and Vikram Shanker and Vladmir Sicca and Hari Sowrirajan and Aidan Swope and Matyas Tamas and Vlad Tenev and Jonathan Thomm and Harold Williams and Lawrence Wu},
      year={2025},
      eprint={2510.01346},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.01346}, 
}
@article{Navott2025,
   abstract = {Gaussian Processes (GPs) provide a flexible and statistically principled foundation for modelling spatiotemporal phenomena, but their $O(N^3)$ scaling makes them intractable for large datasets. Approximate methods such as variational inference (VI), inducing points (sparse GPs), low-rank factorizations (RFFs), local factorizations and approximations (INLA), improve scalability but trade off accuracy or flexibility. We introduce DeepRV, a neural-network surrogate that closely matches full GP accuracy including hyperparameter estimates, while reducing computational complexity to $O(N^2)$, increasing scalability and inference speed. DeepRV serves as a drop-in replacement for GP prior realisations in e.g. MCMC-based probabilistic programming pipelines, preserving full model flexibility. Across simulated benchmarks, non-separable spatiotemporal GPs, and a real-world application to education deprivation in London (n = 4,994 locations), DeepRV achieves the highest fidelity to exact GPs while substantially accelerating inference. Code is provided in the accompanying ZIP archive, with all experiments run on a single consumer-grade GPU to ensure accessibility for practitioners.},
   author = {Jhonathan Navott and Daniel Jenson and Seth Flaxman and Elizaveta Semenova},
   keywords = {cs.LG,stat.ML},
   month = {3},
   title = {DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors},
   url = {https://arxiv.org/abs/2503.21473v2},
   year = {2025}
}
@techReport{Jenson2025,
   abstract = {Neural Processes (NPs) are a rapidly evolving class of models designed to directly model the posterior predictive distribution of stochastic processes. While early architectures were developed primarily as a scalable alternative to Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry applications spanning geology, epidemiology, climate, and robotics. These applications have placed increasing pressure on the scalability of these models, with many architec-tures compromising accuracy for scalability. In this paper, we demonstrate that this tradeoff is often unnecessary, particularly when modeling fully or partially translation invariant processes. We propose a versatile new architecture, the Biased Scan Attention Transformer Neural Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks), group-invariant attention biases, and memory-efficient Biased Scan Attention (BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models while often training in a fraction of the time, (2) exhibit translation invariance, enabling learning at multiple resolutions simultaneously , (3) transparently model processes that evolve in both space and time, (4) support high dimensional fixed effects, and (5) scale gracefully-running inference with over 1M test points with 100K context points in under a minute on a single 24GB GPU.},
   author = {Daniel Jenson and Jhonathan Navott and Piotr Grynfelder and Mengyan Zhang and Makkunda Sharma and Elizaveta Semenova and Seth Flaxman},
   keywords = {cs.LG,stat.ML},
   title = {Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes},
   year = {2025}
}
