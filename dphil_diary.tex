% =============================================================================
% Research Diary - Simplified Version
% =============================================================================
% 
% This document demonstrates how to use the researchdiary.sty package
% to create a professional research diary/journal.
%
% Features:
% - Clean, organized structure
% - Professional black-based design
% - Bibliography support
% - Custom environments for different content types
%
% Author: PhotonZhang
% Email: zyw23@mails.tsinghua.edu.cn
% Collaborator: Claude Sonnet 4 (AI Assistant)
% Version: 1.0
% Release Date: July 29, 2025 (Beijing Time)
% Date: 2025
% =============================================================================

\documentclass[12pt,a4paper,twoside]{article}

% Load the research diary style package
\usepackage{researchdiary}

% Add bibliography file
\addbibliographyfile{ref.bib}

% =============================================================================
% DOCUMENT BEGINS
% =============================================================================

\begin{document}

% Create title page with author name
\maketitlepage{Annabel Jakob}

% Add table of contents
\newpage
\tableofcontents
\newpage

\section{Resources}

\begin{itemize}
	\item \href{https://drive.google.com/drive/folders/1NMMUbx4KKj_sZhrTCWCkbRoOakGAddWG?usp=sharing}{DPhil Progression Information and Resources}
\end{itemize}
\newpage

\section{04 February 2026}

\subsection{Research Plan}
Today's main tasks:
\begin{itemize}[leftmargin=*]
    	\item Meeting with Seth and Gunes
    	\item Familiarise with DPhil milestones and progression requirements
    	\item Reading relevant literature suggested during supervisor meeting
	
\end{itemize}

\subsection{Content Details}

\begin{meetingsummary}
\textbf{Meeting Notes:}
\begin{itemize}
	\item DPhil Milestones
		\begin{itemize}
			\item Term 4, Week 0: Transfer status
			\item 8th/9th Term: Confirmation of DPhil status
		\end{itemize}
	\item Finding a research question
		\begin{itemize}
			\item Bayesian Transformers
				\begin{itemize}
					\item \href{https://arxiv.org/abs/2512.22471}{The Bayesian Geometry of Transformer Attention}
					\item \href{https://arxiv.org/abs/2112.10510}{Transformers Can Do Bayesian Inference}
				\end{itemize}
			\item AI-assisted proofs
				\begin{itemize}
					\item Existing tools: LEAN Proof Checker and \href{https://xenaproject.wordpress.com/what-is-the-xena-project/}{Xena Project}, Autodiff
					\item Possible steps:
						\begin{enumerate}
							\item Compile dataset of theorems presented in previous conference papers (e.g. NeurIPS)
							\item Verify the theorems and proofs in the dataset
							\item Goal: Can we produce \textit{new} theorems and proofs?
						\end{enumerate}
					\item Other resources:
						\begin{itemize}
							\item \url{https://en.wikipedia.org/wiki/Kevin_Buzzard}
							\item \url{https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/}
							\item \href{https://arxiv.org/abs/2510.01346}{Aristotle: IMO-level Automated Theorem Proving}
						\end{itemize}
				\end{itemize}
		\end{itemize}
	\item Mechanistic interpretability
		\begin{itemize}
			\item Potentially connected to encrypted backdoors
			\item Talk to Marek and Junayed
		\end{itemize}
	\item Statistical Machine Learning
		\begin{itemize}
			\item \href{https://arxiv.org/abs/2503.21473}{DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors}
		\end{itemize}
	\item Random Number Generators
		\begin{itemize}
			\item Can a backdoor be hidden in a RNG/ induced through an RNG? I.e. malicious signal induced via carefully chosen "random" numbers?
			\item \href{https://arxiv.org/abs/2204.06974}{Planting Undetectable Backdoors in Machine Learning Models}
			\item \href{https://dl.acm.org/doi/10.1145/3717823.3718245}{Oblivious Defense in ML Models: Backdoor Removal without Detection}
		\end{itemize}
\end{itemize}

\end{meetingsummary}

\section{09 February 2026}
Today's main tasks:
\begin{itemize}[leftmargin=*]
    	\item Read Bayesian Transformers papers:
    	\begin{itemize}
			\item \href{https://arxiv.org/abs/2512.22471}{The Bayesian Geometry of Transformer Attention}
			\item \href{https://arxiv.org/abs/2112.10510}{Transformers Can Do Bayesian Inference}
		\end{itemize}
	
\end{itemize}

\begin{paper}[
	papertitle = {Transformers Can Do Bayesian Inference~\cite{Mller2022}},
	authors = {Mueller et al.},
	summary = {
		Bayesian methods are usually slow or mathematically intractable for large datasets. Deep learning is fast but often bad at uncertainty and priors. As a solution, the authors present Prior-Data Fitted Networks (PFNs), which learn the mapping of the data to Bayesian posterior prediction.}
	]
	\paragraph{Key Contributions}
	\begin{itemize}
		\item PFNs (transformers that learn to approximate Bayesian posterior predictive distributions); this includes architectural changes to the standard transformer architecture, such as the Riemann distribution (a discretized output distribution for regression) and removal of positional encodings for permutation invariance over dataset examples
		\item Demonstrate that PFNs can approximate the PPD of Gaussian processes and Bayesian neural networks (BNNs) + are much faster than standard methods for approximating Bayesian inference (e.g. MCMC, variational inference)
		\item A BNN prior over architectures outperforms XGBoost/CatBoost on small tabular datasets while being $\sim$5000$\times$ faster, with strong calibration (low expected calibration error)
		\item Strong few-shot learning on Omniglot using a simple stroke-based prior
	\end{itemize}
	\tcblower
	\paragraph{Notes}
	\begin{itemize}
		\item Focuses on Bayesian posterior prediction for supervised learning problems, particularly small-data regimes (e.g. 30 training samples in tabular experiments)
		\item PFNs: train transformers to approximate Bayesian posterior predictive distributions by sampling synthetic datasets from a prior, masking labels, and learning to predict them. At inference time, a single forward pass approximates Bayesian inference.
		\item PFNs work for any prior distribution that can be sampled from $\rightarrow$ this is a very relaxed requirement compared to the standard assumptions of other Bayesian inference approximations. E.g. MCMC methods like NUTS~\cite{Hoffman2014} assume access to non-normalised posterior density, ability to evaluate the likelihood $p(D|\theta)$ and prior $p(\theta)$ pointwise, and gradients to the log-posterior.
		\item Deep learning models encode inductive bias through e.g. architecture, training data, and training procedure. PFNs let you inject an explicit prior by defining how to sample synthetic tasks.
		\item Limitation: PFN must be retrained for each new prior
	\end{itemize}

	\paragraph{Thoughts}
	\begin{itemize}
		\item PFNs can effectively mimic Gaussian processes, but what about non-Gaussian processes (especially compared to MCMC and VI)?
		\begin{itemize}
			\item The paper does test beyond GPs---the BNN experiments involve highly non-Gaussian, multimodal posteriors. PFNs handle these well precisely because they only need to sample from the prior, not evaluate densities. The limitation is more about whether you can design a prior that matches your real-world problem.
		\end{itemize}
	\end{itemize}
\end{paper}

\section{10th February 2026}

\begin{summary}
Today's main tasks:
	\begin{itemize}[leftmargin=*]
			\item Meeting with Seth 
	\end{itemize}
\end{summary}

\begin{meetingsummary}
	\textbf{Meeting Notes:}
	\begin{itemize}
		\item Start working on first project in the next 2-4 weeks. Look at conference dates in the next ~6 months and work towards a paper submission deadline. 
		\item What does a DPhil thesis look like? 
			\begin{itemize}
				\item[$\rightarrow$] It's essentially like writing + submitting a Master's thesis on a different topic
				\item[$\rightarrow$] Look at Seth's thesis (although it was essentially an integrated thesis)  
			\end{itemize}
		\item Talk to Dan Jensen about Bayesian wind tunnels and small recursive models 
	\end{itemize}
\end{meetingsummary}

\section{11th February 2026}

\begin{summary}
Today's main tasks:
	\begin{itemize}[leftmargin=*]
			\item Read Bayesian Geometry of Transformer Attention~\cite{Agarwal2026}
	\end{itemize}
\end{summary}

\begin{paper}[
	papertitle = {The Bayesian Geometry of Transformer Attention~\cite{Agarwal2026}},
	authors = {Agarwal et al.},
	summary = {
		Uses \textit{Bayesian wind tunnels} (controlled prediction tasks with closed-form posteriors where the hypothesis space is too large for memorisation and in-context prediction requires genuine probabilistic inference) to empirically prove transformers perform exact Bayesian inference and understand how.
	}
	]
	\tcblower
	\paragraph{Key Contributions}
	\begin{itemize}
		\item Transformers match analytic Bayesian posteriors with $10^{-3}-10^{-4}$ bit accuracy on bijection learning and HMM filtering.
		\item Decompose Bayesian Learning into three primitives: 
			\begin{itemize}
				\item \textit{Belief accummulation}\quad integrating evidence into the posterior as observations arrive
				\item \textit{Belief transport}\quad propagating beliefs forward through stochastic dynamics; i.e. transforming the belief vector according to the system dynamics.
				\item \textit{Random-access binding}\quad Retrieving stored hypotheses by content rather than position.
			\end{itemize}
		\item Architecture comparison: Transformers realise all three; Mamba handles accumulation/transport but struggles with binding; LSTMs only handle accumulation; MLPs fail entirely
	\end{itemize}

	\paragraph{Notes}
	\begin{itemize}
		\item The authors used the different wind tunnels to identify which inference primitive the model realised. Belief accumulation was tested by bijection elimination, belief transport was tested by HMM filtering, and random-access binding was tested by associative recall.
		\item Transformers implement a three-step mechanism
			\begin{enumerate}
				\item \textbf{Layer 0 - Set up representational infrastructure}: Build orthogonal frame (define hypothesis space). Keys at this layer form an approximately orthogonal basis over input tokens. Thus, each hypothesis occupies its own independent direction.
				\item \textbf{Middle layers - Perform actual computation}: Sharpen Q-K alignment (eliminate inconsistent hypotheses)
				\item \textbf{Late layers - Improve numerical accuracy}: Refine value manifold (encode posterior entropy precisely)
			\end{enumerate}
	\end{itemize}

	\paragraph{Thoughts}
	\begin{itemize}
		\item The authors present three inference primitive, but they do not prove that these are \textit{exhaustive}. Different tasks may reveal additional primitives.
		\item Key takeaways:
			\begin{itemize}
				\item Wind tunnel idea: create tasks where you know the true Bayesian answer and memorisation is impossible. This lets us test whether models do genuine inference.
				\item Transformers can do exact Bayes: On the wind tunnel tasks, transformers match the analytic posterior with high precision.
				\item Architectures differ in what inference they can perform: The primitives framework gives us a vocabulary for why transformers beat LSTMs on some tasks (because attention enables content-based retrieval that recurrence cannot).
				\item Attention creates interpretable geometric structure: Keys become orthogonal hypothesis axes, attention sharpens onto valid hypotheses across depth. 
			\end{itemize}
	\end{itemize}
\end{paper}
% Print bibliography section
\printbibliographysection

\end{document} 