% =============================================================================
% Research Diary - Simplified Version
% =============================================================================
% 
% This document demonstrates how to use the researchdiary.sty package
% to create a professional research diary/journal.
%
% Features:
% - Clean, organized structure
% - Professional black-based design
% - Bibliography support
% - Custom environments for different content types
%
% Author: PhotonZhang
% Email: zyw23@mails.tsinghua.edu.cn
% Collaborator: Claude Sonnet 4 (AI Assistant)
% Version: 1.0
% Release Date: July 29, 2025 (Beijing Time)
% Date: 2025
% =============================================================================

\documentclass[12pt,a4paper,twoside]{article}

% Load the research diary style package
\usepackage{researchdiary}

% Add bibliography file
\addbibliographyfile{ref.bib}

% =============================================================================
% DOCUMENT BEGINS
% =============================================================================

\begin{document}

% Create title page with author name
\maketitlepage{Annabel Jakob}{DPhil: Computer Science}

% Add table of contents
\newpage
\tableofcontents
\newpage

\section{Resources}

\begin{itemize}
	\item \href{https://drive.google.com/drive/folders/1NMMUbx4KKj_sZhrTCWCkbRoOakGAddWG?usp=sharing}{DPhil Progression Information and Resources}
\end{itemize}
\newpage

\section{04 February 2026}

\subsection{Research Plan}
Today's main tasks:
\begin{itemize}[leftmargin=*]
	\item Meeting with Seth and Gunes
	\item Familiarise with DPhil milestones and progression requirements
	\item Reading relevant literature suggested during supervisor meeting

\end{itemize}

\subsection{Content Details}

\begin{meetingsummary}
	\textbf{Meeting Notes:}
	\begin{itemize}
		\item DPhil Milestones
		      \begin{itemize}
			      \item Term 4, Week 0: Transfer status
			      \item 8th/9th Term: Confirmation of DPhil status
		      \end{itemize}
		\item Finding a research question
		      \begin{itemize}
			      \item Bayesian Transformers
			            \begin{itemize}
				            \item \href{https://arxiv.org/abs/2512.22471}{The Bayesian Geometry of Transformer Attention}
				            \item \href{https://arxiv.org/abs/2112.10510}{Transformers Can Do Bayesian Inference}
			            \end{itemize}
			      \item AI-assisted proofs
			            \begin{itemize}
				            \item Existing tools: LEAN Proof Checker and \href{https://xenaproject.wordpress.com/what-is-the-xena-project/}{Xena Project}, Autodiff
				            \item Possible steps:
				                  \begin{enumerate}
					                  \item Compile dataset of theorems presented in previous conference papers (e.g. NeurIPS)
					                  \item Verify the theorems and proofs in the dataset
					                  \item Goal: Can we produce \textit{new} theorems and proofs?
				                  \end{enumerate}
				            \item Other resources:
				                  \begin{itemize}
					                  \item \url{https://en.wikipedia.org/wiki/Kevin_Buzzard}
					                  \item \url{https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/}
					                  \item \href{https://arxiv.org/abs/2510.01346}{Aristotle: IMO-level Automated Theorem Proving}
				                  \end{itemize}
			            \end{itemize}
		      \end{itemize}
		\item Mechanistic interpretability
		      \begin{itemize}
			      \item Potentially connected to encrypted backdoors
			      \item Talk to Marek and Junayed
		      \end{itemize}
		\item Statistical Machine Learning
		      \begin{itemize}
			      \item \href{https://arxiv.org/abs/2503.21473}{DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors}
		      \end{itemize}
		\item Random Number Generators
		      \begin{itemize}
			      \item Can a backdoor be hidden in a RNG/ induced through an RNG? I.e. malicious signal induced via carefully chosen "random" numbers?
			      \item \href{https://arxiv.org/abs/2204.06974}{Planting Undetectable Backdoors in Machine Learning Models}
			      \item \href{https://dl.acm.org/doi/10.1145/3717823.3718245}{Oblivious Defense in ML Models: Backdoor Removal without Detection}
		      \end{itemize}
	\end{itemize}

\end{meetingsummary}

\section{09 February 2026}
Today's main tasks:
\begin{itemize}[leftmargin=*]
	\item Read Bayesian Transformers papers:
	      \begin{itemize}
		      \item \href{https://arxiv.org/abs/2512.22471}{The Bayesian Geometry of Transformer Attention}
		      \item \href{https://arxiv.org/abs/2112.10510}{Transformers Can Do Bayesian Inference}
	      \end{itemize}

\end{itemize}

\subsection{Paper - Transformers Can Do Bayesian Inference}
\begin{paper}[
		papertitle = {Transformers Can Do Bayesian Inference~\cite{Mller2022}},
		authors = {Mueller et al.},
		summary = {
				Bayesian methods are usually slow or mathematically intractable for large datasets. Deep learning is fast but often bad at uncertainty and priors. As a solution, the authors present Prior-Data Fitted Networks (PFNs), which learn the mapping of the data to Bayesian posterior prediction.}
	]
	\paragraph{Key Contributions}
	\begin{itemize}
		\item PFNs (transformers that learn to approximate Bayesian posterior predictive distributions); this includes architectural changes to the standard transformer architecture, such as the Riemann distribution (a discretized output distribution for regression) and removal of positional encodings for permutation invariance over dataset examples
		\item Demonstrate that PFNs can approximate the PPD of Gaussian processes and Bayesian neural networks (BNNs) + are much faster than standard methods for approximating Bayesian inference (e.g. MCMC, variational inference)
		\item A BNN prior over architectures outperforms XGBoost/CatBoost on small tabular datasets while being $\sim$5000$\times$ faster, with strong calibration (low expected calibration error)
		\item Strong few-shot learning on Omniglot using a simple stroke-based prior
	\end{itemize}
	\tcblower
	\paragraph{Notes}
	\begin{itemize}
		\item Focuses on Bayesian posterior prediction for supervised learning problems, particularly small-data regimes (e.g. 30 training samples in tabular experiments)
		\item PFNs: train transformers to approximate Bayesian posterior predictive distributions by sampling synthetic datasets from a prior, masking labels, and learning to predict them. At inference time, a single forward pass approximates Bayesian inference.
		\item PFNs work for any prior distribution that can be sampled from $\rightarrow$ this is a very relaxed requirement compared to the standard assumptions of other Bayesian inference approximations. E.g. MCMC methods like NUTS~\cite{Hoffman2014} assume access to non-normalised posterior density, ability to evaluate the likelihood $p(D|\theta)$ and prior $p(\theta)$ pointwise, and gradients to the log-posterior.
		\item Deep learning models encode inductive bias through e.g. architecture, training data, and training procedure. PFNs let you inject an explicit prior by defining how to sample synthetic tasks.
		\item Limitation: PFN must be retrained for each new prior
	\end{itemize}

	\paragraph{Thoughts}
	\begin{itemize}
		\item PFNs can effectively mimic Gaussian processes, but what about non-Gaussian processes (especially compared to MCMC and VI)?
		      \begin{itemize}
			      \item The paper does test beyond GPs---the BNN experiments involve highly non-Gaussian, multimodal posteriors. PFNs handle these well precisely because they only need to sample from the prior, not evaluate densities. The limitation is more about whether you can design a prior that matches your real-world problem.
		      \end{itemize}
	\end{itemize}
\end{paper}

\section{10th February 2026}

\begin{summary}
	Today's main tasks:
	\begin{itemize}[leftmargin=*]
		\item Meeting with Seth
	\end{itemize}
\end{summary}

\subsection{Supervisor Meeting}
\begin{meetingsummary}
	\textbf{Meeting Notes:}
	\begin{itemize}
		\item Start working on first project in the next 2-4 weeks. Look at conference dates in the next ~6 months and work towards a paper submission deadline.
		\item What does a DPhil thesis look like?
		      \begin{itemize}
			      \item[$\rightarrow$] It's essentially like writing + submitting a Master's thesis on a different topic
			      \item[$\rightarrow$] Look at Seth's thesis (although it was essentially an integrated thesis)
		      \end{itemize}
		\item Talk to Dan Jensen about Bayesian wind tunnels and small recursive models
	\end{itemize}
\end{meetingsummary}

\section{11th February 2026}

\begin{summary}
	Today's main tasks:
	\begin{itemize}[leftmargin=*]
		\item Read Bayesian Geometry of Transformer Attention~\cite{Agarwal2026}
	\end{itemize}
\end{summary}

\subsection{Paper - The Bayesian Geometry of Transformer Attention}
\begin{paper}[
		papertitle = {The Bayesian Geometry of Transformer Attention~\cite{Agarwal2026}},
		authors = {Agarwal et al.},
		summary = {
				Uses \textit{Bayesian wind tunnels} (controlled prediction tasks with closed-form posteriors where the hypothesis space is too large for memorisation and in-context prediction requires genuine probabilistic inference) to empirically prove transformers perform exact Bayesian inference and understand how.
			}
	]
	\tcblower
	\paragraph{Key Contributions}
	\begin{itemize}
		\item Transformers match analytic Bayesian posteriors with $10^{-3}-10^{-4}$ bit accuracy on bijection learning and HMM filtering.
		\item Decompose Bayesian Learning into three primitives:
		      \begin{itemize}
			      \item \textit{Belief accumulation}\quad integrating evidence into the posterior as observations arrive
			      \item \textit{Belief transport}\quad propagating beliefs forward through stochastic dynamics; i.e. transforming the belief vector according to the system dynamics.
			      \item \textit{Random-access binding}\quad Retrieving stored hypotheses by content rather than position.
		      \end{itemize}
		\item Architecture comparison: Transformers realise all three; Mamba handles accumulation/transport but struggles with binding; LSTMs only handle accumulation; MLPs fail entirely
	\end{itemize}

	\paragraph{Notes}
	\begin{itemize}
		\item The authors used the different wind tunnels to identify which inference primitive the model realised. Belief accumulation was tested by bijection elimination, belief transport was tested by HMM filtering, and random-access binding was tested by associative recall.
		\item Transformers implement a three-step mechanism
		      \begin{enumerate}
			      \item \textbf{Layer 0 - Set up representational infrastructure}: Build orthogonal frame (define hypothesis space). Keys at this layer form an approximately orthogonal basis over input tokens. Thus, each hypothesis occupies its own independent direction.
			      \item \textbf{Middle layers - Perform actual computation}: Sharpen Q-K alignment (eliminate inconsistent hypotheses)
			      \item \textbf{Late layers - Improve numerical accuracy}: Refine value manifold (encode posterior entropy precisely)
		      \end{enumerate}
	\end{itemize}

	\paragraph{Thoughts}
	\begin{itemize}
		\item The authors present three inference primitive, but they do not prove that these are \textit{exhaustive}. Different tasks may reveal additional primitives.
		\item Key takeaways:
		      \begin{itemize}
			      \item Wind tunnel idea: create tasks where you know the true Bayesian answer and memorisation is impossible. This lets us test whether models do genuine inference.
			      \item Transformers can do exact Bayes: On the wind tunnel tasks, transformers match the analytic posterior with high precision.
			      \item Architectures differ in what inference they can perform: The primitives framework gives us a vocabulary for why transformers beat LSTMs on some tasks (because attention enables content-based retrieval that recurrence cannot).
			      \item Attention creates interpretable geometric structure: Keys become orthogonal hypothesis axes, attention sharpens onto valid hypotheses across depth.
		      \end{itemize}
	\end{itemize}
\end{paper}

\section{16th February 2026}

\begin{summary}
	Today's main tasks:
	\begin{itemize}[leftmargin=*]
		\item Read Aristotle: IMO-level Automated Theorem Proving~\cite{Achim2025}
	\end{itemize}
\end{summary}

\subsection{Paper - Aristotle: IMO-level Automated Theorem Proving }
\begin{paper}[
		papertitle = {Aristotle: IMO-level Automated Theorem Proving~\cite{Achim2025}},
		authors = {Achim et al.},
		summary = {
				Aristotle is a system for automated theorem proving that can solve International Mathematical Olympiad (IMO) level problems. It uses a combination of formal verification and informal reasoning, which allows for flexible planning and reasoning and grants access to a much large corpus of mathematical proofs.
			}
	]
	\tcblower
	\paragraph{Key Contributions}
	\begin{itemize}
		\item Aristotle: system for automated theorem proving that achieves gold-medal performance on 2025 IMO problems
	\end{itemize}

	\paragraph{Notes}
	\begin{itemize}
		\item Aristotle combines formal verification with informal reasoning
		      \begin{itemize}
			      \item This allows it to leverage the large corpus of informal mathematical proofs + more flexibility in reasoning and planning (compared to fully formal systems like LEAN)
		      \end{itemize}
		\item Aristotle is composed of three main components:
		      \begin{itemize}
			      \item \textit{Lean proof search algorithm:}\quad Core engine that builds proofs step-by-step using Monte Carlo Graph Search (MCGS)
			            \begin{itemize}
				            \item A model trained using reinforcement learning serves two purposed during search: 1. Suggest which tactics to try from a given proof state and 2. estimate how likely a state is to be provable. This guides the search by predicting which proof steps are the most promising.
			            \end{itemize}
			      \item \textit{Informal reasoning system:}\quad Uses a language model to generate an informal proof sketch and breaks this proof down into lemmas which are then auto-formalized into LEAN.
			      \item \textit{Geometry solver:}\quad Specialized C++ engine for solving plane geometry problems.
		      \end{itemize}
		\item Aristotle workflow:
		      \begin{enumerate}
			      \item Generate informal proof for target theorem
			      \item Break informal proof into lemmas
			      \item Auto-formalize lemmas into LEAN
			      \item Proof search using MCGS
			      \item If the proof search fails, revise the lemmas and repeat from step 3 until success or other termination condition.
		      \end{enumerate}
	\end{itemize}

	\paragraph{Follow-up Reading}
	\begin{itemize}
		\item \href{https://xenaproject.wordpress.com/what-is-the-xena-project/}{Xena Project}
		\item Monte Carlo Graph Search (MCGS)/ Monte Carlo Tree Search (MCTS)
		\item \href{https://lean-lang.org/}{LEAN Proof Assistant}
		\item \href{https://deepmind.google/research/alphazero-and-muzero/}{AlphaZero and MuZero}
	\end{itemize}
\end{paper}

\section{17th February 2026}
\begin{summary}
	Today's main tasks:
	\begin{itemize}[leftmargin=*]
		\item Read DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors~\cite{Navott2025}
		\item Create slides for supervisor meeting
	\end{itemize}
\end{summary}

\subsection{Paper - DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors}
\begin{paper}[
	papertitle={DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors~\cite{Navott2025}}
	authors={Navott et al.}
	summary={DeepRV is a decoder-only neural surrogate that replaces GP prior sampling in MCMC pipelines by learning the mapping from latent noise and kernel parameters directly to function values, achieving near-exact GP accuracy at substantially reduced computational cost.}
	]

	\tcblower

	\paragraph{Key Contributions}
	\begin{itemize}
		\item Novel decoder-only architecture: Eliminates the VAE encoder entirely, removing posterior collapse and oversmoothing errors that plague prior surrogates like PriorCVAE.
		\item Three architecture variants: MLP (baseline), gMLP (best accuracy/ efficiency trade-off), and Transformer (handles variable-length location sets via kernel-based attention).
		\item State-of-the-art fidelity: Outperforms INLA, inducing points, RFFs, ADVI, and PriorCVAE in matching full GP MCMC, on both predictive and hyperparameter recovery metrics.
		\item Flexible kernel support: Handles non-separable spatiotemporal kernals that INLA and RFFs cannot natively accommmodate.
		\item Practical scalability: Demonstrated on real-world city-scale dataset ($n=4,994$ London LSOAs), completing in approx. 3 hours versus approx. 70 hours for full GP, on a single consumer GPU.
	\end{itemize}

	\paragraph{Notes}
	\begin{itemize}
		\item \textbf{Problem:} Gaussian Processes (GPs) are the gold standard for spatiotemporal modelling but scale as $\mathcal{O}(N^3)$, making them intractable for large datasets. Existing approximations (INLA, sparce/inducing-point GPs, RFFs, VAE-based surrogates like PriorCVAE) trade accuracy for scalability.
		\item \textbf{Core idea:} Decoder-only neural surrogate exploiting the GP Cholesky decomposition $\mathbf{f}=L\mathbf{z}$, learning the mapping $(\mathbf{\theta}, \mathbf{z})\rightarrow \hat{f}$ directly. This eliminates the VAE encoder, thus removing posterior collapse and oversmoothing errors.
		\item \textbf{Drop-in replacement:} Plugs into MCMC pipelines in place of GP prior sampling and therefore reduces complexity to $\mathcal{0}(M^2)$.
		\item \textbf{Results:} Outperforms INLA, inducing points, RFFs, ADVI, and PriorCVAE on predictive MSE and hyperparameter recovery (Wasserstein distance to GP posteriors)
		\item \textbf{Flexibility:} Handles non-separable spatiotemporal kernels unsupported by INLA/RFFs.
		\item \textbf{Real-world:} London LSOA $(n=4,994)$ completed in approx. 3 hours vs. approx. 70 hours for full GP, on a single consumer GPU.
		\item Limitations: Pre-training data generation still requires $\mathcal{O}(N^3)$ Cholesky decompositions; gMLP requires fixed grid size at training time.
	\end{itemize}

	\paragraph{Keywords}
	\begin{itemize}
		\item Gaussian Processes
		\item Kernel functions
	\end{itemize}
\end{paper}

\section{18th February 2026}

\subsection{Current Status}
\subsubsection*{Thoughts so far}
\begin{itemize}
	\item Found the Bayesian Transfomer~\cite{Mller2022,Agarwal2026} and DeepRV~\cite{Navott2025} papers most promising.
	\item Aristotle~\cite{Achim2025} was interesting, but I didn't immediately have an intuition for possible extensions and the space seems somewhat saturated (the paper mentions another system developed independently at the same time as Aristotle which is very similar to their own).
	\item There should be a way to combine the Bayesian Transformer and DeepRV papers.
\end{itemize}

\subsubsection*{Reading Notes}
\paragraph{Transformers Can Do Bayesian Inference~\cite{Mller2022}}
\begin{itemize}
	\item Propose \textit{Prior-Data Fitted Networks} (PFNs)
	\item[$\rightarrow$] framework for amortising Bayesian Inference via in-context learning
	\item[$\rightarrow$] Transformer is meta-trained on dataset sampled from prior and learns to approximate the posterior predictive distribution in a single forward pass
\end{itemize}

\paragraph{The Bayesian Geometry of Transformer Attention~\cite{Agarwal2026}}
\begin{itemize}
	\item Uses Bayesian wind tunnels (tasks with known analytic posteriors and where memorisation is impossible) to empirically show that small transformers achieve near-exact Bayesian posteriors
	\item Decomposes Bayesian inference into three primitives (belief accumulation, belief transport, random-access binding) and shows which architectures (Transformers, Mamba, LSTM, MLP) realises which primitives + geometric analysis of the internal mechanisms (orthogonal key bases, progressive QK sharpening, value manifolds)
\end{itemize}

\paragraph{DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors~\cite{Navott2025}}
\begin{itemize}
	\item Proposes a neural surrogate for Gaussian Processes in spatiotemporal inference using a decoder-only architecture that directly learns the Cholesky factor mapping.
\end{itemize}

\subsubsection{Project Ideas}
\begin{itemize}
	\item The Bayesian Geometry paper defines three primitives that correspond to the tasks that they used $\rightarrow$ the list of primitives is not necessarily exhaustive and we could define additional primitives for tasks not included in the paper
	\item The Bayesian Geometry paper only tests standard transformers, Mamba, LSTMs, and MLPs. We could test additional architectures
	\item DeepRV replaces GP priors in MCMC pipelines but still requires MCMC for inference $\rightarrow$ could combine DeepRV and PFNs so that DeepRV handles the prior and PFNs replace MCMC
	\item[$\rightarrow$] verify through wind tunnels
	\item[$\rightarrow$] for PFN, the prior over functions would be defined by DeepRV
\end{itemize}

\subsection{Supervisor Meeting}

\begin{meetingsummary}
	\paragraph{Paper Discussion}
	\begin{itemize}
		\item Bayesian Geometry
		\begin{itemize}
			\item How are the models trained? How are they evaluated?
			\item What is the distribution over?
			\item G\" une\c s used to work with \href{https://pyprop.readthedocs.io/en/latest/introduction.html}{pyprop} $\rightarrow$ big overlap with what the Bayesian Geometry paper did
			\item DeepRV without MCMC was already done. See BSA-TNP~\cite{Jenson2025}
			\item Talk to Dan Jensen about this
		\end{itemize}
		\item PFNs
		\begin{itemize}
			\item how do they get from a prior-data fitted network to the posterior?
		\end{itemize}
	\end{itemize}

	\paragraph{Notes}
	\begin{itemize}
		\item Look at fundamental computational Bayesian Theory:
		      \begin{itemize}
				\item \href{https://users.aalto.fi/~ave/}{Aki Vehtari homepage}
				\item \href{https://bayes.wustl.edu/etj/prob/book.pdf}{Probability Theory: The Logic of Science}
				\item \href{https://sites.stat.columbia.edu/gelman/book/}{Bayesian Data Analysis}
				\item \href{https://en.wikipedia.org/wiki/David_J._C._MacKay}{David J. C. MacKay Wiki Page}
				\item \href{https://www.inference.org.uk/itprnn/book.pdf}{Information Theory, Inference, and Learning Algorithms}
		      \end{itemize}
		\item Evaluate BSA-TNP and/or DeepRV using wind tunnel
		\item[$\rightarrow$] G\" une\c s has a lot of data for this (particle physics) 
		\item[$\rightarrow$] See \href{https://arxiv.org/abs/2002.09309}{Efficiently Sampling Functions from Gaussian Process Posteriors}
	\end{itemize}

	
\end{meetingsummary}

\section{20th February 2026}
\begin{summary}	
	Today's main tasks:
	\begin{itemize}[leftmargin=*]
		\item Review BSPP course materials and other Bayesian inference resources suggested by supervisor meeting
	\end{itemize}
\end{summary}

\section{21st February 2026}
\begin{summary}
	Today's main tasks:
	\begin{itemize}[leftmargin=*]
		\item Review BSPP lectures 3 and 4
		\item Re-read Transformers Can Do Bayesian Inference~\cite{Mller2022} with a focus on understanding the training and evaluation procedures in more detail
	\end{itemize}
\end{summary}

\subsection{Paper: Transformers Can Do Bayesian Inference}

\begin{paper}[
		papertitle = {Transformers Can Do Bayesian Inference~\cite{Mller2022}},
		authors = {Mueller et al.},
		summary = {
				Bayesian methods are usually slow or mathematically intractable for large datasets. Deep learning is fast but often bad at uncertainty and priors. As a solution, the authors present Prior-Data Fitted Networks (PFNs), which learn the mapping of the data to Bayesian posterior prediction.}
	]
	Github repo: \url{https://github.com/automl/TransformersCanDoBayesianInference}
	\paragraph{Questions to clarify}
	\begin{itemize}
		\item How do they get from a prior-data fitted network to the posterior?
	\end{itemize}

	\tcblower
	\paragraph{Notes}
	\begin{itemize}
		\item Method for \textit{posterior approximation} formulated as a supervised classification problem
		\item[$\rightarrow$] i.e. the goal is to find a model that approximates the \textit{posterior predictive distribution} 
		\item[$\rightarrow$] Authors consider a prior $p(t)$ where the latent variable $t$ is the task. 
		\item The training procedure is exactly the same as standard supervised learning. The key difference is that the training data is generated by sampling synthetic datasets from a prior distribution, masking the labels, and learning to predict them. Thus, the model learns to map from datasets to posterior predictions.
		\item[$\rightarrow$] For each training step, samples a dataset $D = \{(x_i, y_i)\}^n_{i=1}$ (these are the \textbf{context tokens}) and query tokens $x$ from the same prior distribution $p(\mathcal{D})$. The label of the query is \textit{hidden} (zeroed out or set to a special mask token). The loss is then computed only for the query tokens.
		\item[$\rightarrow$] Uses \textit{Prior-Data Negative Log-Likelihood} (Prior-Data NLL): \[ \ell_\theta = \mathbb{E}_{D \cup \{ x, y\} \tilde{p(\mathcal{D})} }[- \log q_\theta (y | x, D)] \] where $D \cup \{ x, y\}$ is a dataset of size $|D| + 1 $ sampled from $p(\mathcal{D})$
		\item[$\rightarrow$] $\ell_\theta$ is equal to the expectation of the cross-entropy between the PDD and the approximation $q_\theta$. 
		\item \textbf{Meta-learning / Learning-to-lean}: Goal is to generalize learning methods from a training set of datasets to a validation set of datasets
		\item Architecture: Based on Transformer encoder without positional encoding (to ensure permutation invariance over dataset examples)
		\item Conducted the experiments using \textit{Riemann Distributions}, i.e. discretized continuous distribution
		\item Experiments approximated Gaussian Processes and Bayesian Neural Networks
		\item Experiments on real-world datasets:
		\begin{itemize}
			\item Tabular classification: Use a GP prior with hyper-priors (distribution over hyperparameters). To make the dataset a classification prior, the authors computed the median of all targets in the dataset and set the class to one if greater than the median and zero otherwise.
			\item Use BNN to learn a prior over model architectures. Authors define the prior probability over the space of model architectures (see \cite{Mller2022} for sampling procedure)
		\end{itemize}
	\end{itemize}
\end{paper}

\section{23rd February 2026}

\subsection{Paper: The Bayesian Geometry of Transformer Attention}
\begin{paper}
	papertitle = {The Bayesian Geometry of Transformer Attention~\cite{Agarwal2026}},
	authors = {Agarwal et al.},
	summary = {
			Uses \textit{Bayesian wind tunnels} (controlled prediction tasks with closed-form posteriors where the hypothesis space is too large for memorisation and in-context prediction requires genuine probabilistic inference) to empirically prove transformers perform exact Bayesian inference and understand how.
		}
	\paragraph{Questions to clarify}
	\begin{itemize}
		\item How are the models trained? How are they evaluated?
		\item What is the distribution over?
	\end{itemize}
	\tcblower
	\paragraph{Notes}
	\begin{itemize}
		\item Setup
		\begin{itemize}
			\item Given a family of tasks indexed by $\theta \tilde{\pi(\theta)}$, inputs $x$ are drawn from some distribution, labels are drawn according to $y\tilde{p(y|x, \theta)}$. The model observes context $c=\{ (x_i, y_i)\}^k_{i=1}$ and must predict $y$ for a new query input.
			\item Loss function is the population cross-entropy, just as in \cite{Mller2022} (here, the authors sample a new task for each batch though)
 		\end{itemize}
		\item Evaluation
		\begin{itemize}
			\item "A model that achieves the correct posterior entropy at every position is functionally Bayesian - it produces predictions with the same uncertainty profile as the exact posterior"
			\item In general, entropy matching doesn't guarantee distributional equivalence since two very different distributions can have the same entropy
			\item[$\rightarrow$] just measuring entropy MAE doesn't tell us if the model had learned the right distribution, only the right uncertainty level.
			\item[$\rightarrow$] The authors then argue that in their specific tasks, entropy does act as a sufficient diagnostic because the constrained structure of the task severely limits which distributions are consistent with a given entropy value. In addition, they verify distribution matching using KL divergence
			\item[$\rightarrow$] The main metric used is the \textit{entropy calibration error}: \[ MAE = \frac{1}{L}\sum_k |H_\text{model}(k)-H_\text{Bayes}(k)|\]  
		\end{itemize}
		\item How Transformers perform exact Bayesian inference
		\begin{itemize}
			\item They first construct a representational frame, execute sequential elimination within this frame, and then refine posterior precision across layers.
		\end{itemize}
	\end{itemize}
	\paragraph{Thoughts}
	\begin{itemize}
		\item Authors use small transformers. Do the findings translate to larger models?
	\end{itemize}
\end{paper}

% Print bibliography section
\printbibliographysection

\end{document}