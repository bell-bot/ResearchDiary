% =============================================================================
% Research Diary - Simplified Version
% =============================================================================
% 
% This document demonstrates how to use the researchdiary.sty package
% to create a professional research diary/journal.
%
% Features:
% - Clean, organized structure
% - Professional black-based design
% - Bibliography support
% - Custom environments for different content types
%
% Author: PhotonZhang
% Email: zyw23@mails.tsinghua.edu.cn
% Collaborator: Claude Sonnet 4 (AI Assistant)
% Version: 1.0
% Release Date: July 29, 2025 (Beijing Time)
% Date: 2025
% =============================================================================

\documentclass[12pt,a4paper,twoside]{article}

% Load the research diary style package
\usepackage{researchdiary}

% Add bibliography file
\addbibliographyfile{ref.bib}

% =============================================================================
% DOCUMENT BEGINS
% =============================================================================

\begin{document}

% Create title page with author name
\maketitlepage{Annabel Jakob}

% Add table of contents
\newpage
\tableofcontents
\newpage

\section{Resources}

\begin{itemize}
	\item \href{https://drive.google.com/drive/folders/1NMMUbx4KKj_sZhrTCWCkbRoOakGAddWG?usp=sharing}{DPhil Progression Information and Resources}
\end{itemize}
\newpage

\section{04 February 2026}

\subsection{Research Plan}
Today's main tasks:
\begin{itemize}[leftmargin=*]
    	\item Meeting with Seth and Gunes
    	\item Familiarise with DPhil milestones and progression requirements
    	\item Reading relevant literature suggested during supervisor meeting
	
\end{itemize}

\subsection{Content Details}

\begin{meetingsummary}
\textbf{Meeting Notes:}
\begin{itemize}
	\item DPhil Milestones
		\begin{itemize}
			\item Term 4, Week 0: Transfer status
			\item 8th/9th Term: Confirmation of DPhil status
		\end{itemize}
	\item Finding a research question
		\begin{itemize}
			\item Bayesian Transformers
				\begin{itemize}
					\item \href{https://arxiv.org/abs/2512.22471}{The Bayesian Geometry of Transformer Attention}
					\item \href{https://arxiv.org/abs/2112.10510}{Transformers Can Do Bayesian Inference}
				\end{itemize}
			\item AI-assisted proofs
				\begin{itemize}
					\item Existing tools: LEAN Proof Checker and \href{https://xenaproject.wordpress.com/what-is-the-xena-project/}{Xena Project}, Autodiff
					\item Possible steps:
						\begin{enumerate}
							\item Compile dataset of theorems presented in previous conference papers (e.g. NeurIPS)
							\item Verify the theorems and proofs in the dataset
							\item Goal: Can we produce \textit{new} theorems and proofs?
						\end{enumerate}
					\item Other resources:
						\begin{itemize}
							\item \url{https://en.wikipedia.org/wiki/Kevin_Buzzard}
							\item \url{https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/}
							\item \href{https://arxiv.org/abs/2510.01346}{Aristotle: IMO-level Automated Theorem Proving}
						\end{itemize}
				\end{itemize}
		\end{itemize}
	\item Mechanistic interpretability
		\begin{itemize}
			\item Potentially connected to encrypted backdoors
			\item Talk to Marek and Junayed
		\end{itemize}
	\item Statistical Machine Learning
		\begin{itemize}
			\item \href{https://arxiv.org/abs/2503.21473}{DeepRV: Accelerating spatiotemporal inference with pre-trained neural priors}
		\end{itemize}
	\item Random Number Generators
		\begin{itemize}
			\item Can a backdoor be hidden in a RNG/ induced through an RNG? I.e. malicious signal induced via carefully chosen "random" numbers?
			\item \href{https://arxiv.org/abs/2204.06974}{Planting Undetectable Backdoors in Machine Learning Models}
			\item \href{https://dl.acm.org/doi/10.1145/3717823.3718245}{Oblivious Defense in ML Models: Backdoor Removal without Detection}
		\end{itemize}
\end{itemize}

\end{meetingsummary}

\section{09 February 2026}
Today's main tasks:
\begin{itemize}[leftmargin=*]
    	\item Read Bayesian Transformers papers:
    	\begin{itemize}
			\item \href{https://arxiv.org/abs/2512.22471}{The Bayesian Geometry of Transformer Attention}
			\item \href{https://arxiv.org/abs/2112.10510}{Transformers Can Do Bayesian Inference}
		\end{itemize}
	
\end{itemize}

\begin{paper}[
	papertitle = {Transformers Can Do Bayesian Inference~\cite{Mller2022}},
	authors = {Mueller et al.},
	summary = {
		Bayesian methods are usually slow or mathematically intractable for large datasets. Deep learning is fast but often bad at uncertainty and priors.
		As a solution, the authors present Prior-Data Fitted Networks (PFNs), which learn the mapping of the data to Bayesian posterior prediction.}
	]
	\tcblower
	\begin{itemize}
		\item Item 1
	\end{itemize}
\end{paper}

% Print bibliography section
\printbibliographysection

\end{document} 